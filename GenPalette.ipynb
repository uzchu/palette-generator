{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_name = 'voyagefox_'\n",
    "file_path = f'./json/{account_name}.json'\n",
    "\n",
    "with open(file_path, 'r') as json_fp:\n",
    "    palette_dict = json.load(json_fp)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "account_name = 'voyagefox_'\n",
    "file_path = f'../data/{account_name}.json'\n",
    "\n",
    "with open(file_path, 'r') as json_fp:\n",
    "    palette_dict = json.load(json_fp)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> original picture : voyagefox__055_3772Likes.png\n",
      ">>> back_cols : ([65, 67, 70], [92, 84, 81], [150, 122, 108])\n",
      ">>> fore_cols : ([36, 44, 49], [61, 62, 65])\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the loaded json\n",
    "# palette_dict = {\"voyagefox__055_3772Likes.png\": [[[65, 67, 70], [92, 84, 81], [150, 122, 108]], [[36, 44, 49], [61, 62, 65]]], ...}\n",
    "\n",
    "for item in palette_dict.items():\n",
    "    file_name = item[0]\n",
    "    back_cols = tuple(item[1][0])\n",
    "    fore_cols = tuple(item[1][1])\n",
    "    \n",
    "    print(f'>>> original picture : {file_name}')\n",
    "    print(f'>>> back_cols : {back_cols}')\n",
    "    print(f'>>> fore_cols : {fore_cols}\\n')\n",
    "    break\n",
    "\n",
    "\n",
    "# visualize one sample pair\n",
    "back_cols_img = np.expand_dims(np.array(back_cols), 0)\n",
    "fore_cols_img = np.expand_dims(np.array(fore_cols), 0)\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(back_cols_img)\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.imshow(fore_cols_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use PIL Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "<PIL.Image.Image image mode=RGB size=1000x1 at 0x2B4E7BD9320>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAABCAIAAADCYhNkAAAAGklEQVR4nO3BMQEAAADCoPVPbQ0PoAAAgHsDC7kAAfTKO28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1000x1 at 0x2B4E7BD9320>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate new background - Image.new(mode,size,color)\n",
    "im=Image.new('RGB',(1000,1),(0,0,0))\n",
    "print(type(im))\n",
    "print(im)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAABCAIAAADCYhNkAAAALUlEQVR4nO3UwQkAAAzCwLr/0HYJpVByE/gIygPkibDQQFgosHU9AQ9p+CvkLdtWBgB8D/z0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1000x1 at 0x2B4E7BD9320>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colorize the background\n",
    "draw=ImageDraw.Draw(im)\n",
    "draw.rectangle((0*200, 0, 1*200, 1), fill=(255, 0, 0))\n",
    "draw.rectangle((1*200, 0, 2*200, 1), fill=(0, 255, 0))\n",
    "draw.rectangle((2*200, 0, 3*200, 1), fill=(0, 0, 255))\n",
    "draw.rectangle((3*200, 0, 4*200, 1), fill=(255, 255, 0))\n",
    "draw.rectangle((4*200, 0, 5*200, 1), fill=(0, 255, 255))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAABCAIAAADCYhNkAAAAOUlEQVR4nGN0dHZjGAWjgNpAWpB7oJ0wCoYhsFKTHmgnjIJhCPo2HR1oJ4yCYQgkhQQG2gmjYBgCALp6A75NcVg0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1000x1 at 0x2B4E7BD9320>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colorize the background - using each palette in dataset\n",
    "draw=ImageDraw.Draw(im)\n",
    "batch = 200\n",
    "for j, c in enumerate(back_cols + fore_cols):\n",
    "    c = tuple(c)\n",
    "    draw.rectangle((j*batch,0,(j+1)*batch,1),fill=c)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading function, generator for training \n",
    "\n",
    "During the generation process, the original color scheme is used as the label. The foreground colors will be concealed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataset(account_name, output_x_num=3, output_y_num=2):  # x == back_cols, y == fore_cols\n",
    "    #file_path = f'../data/{account_name}.json'\n",
    "    file_path = f'./json/{account_name}.json'\n",
    "\n",
    "    with open(file_path, 'r') as json_fp:\n",
    "        palette_dict = json.load(json_fp)\n",
    "    print(f'>>> finished loading {len(palette_dict)} palettes')\n",
    "    \n",
    "    dataAB = []\n",
    "    for back_cols, fore_cols in palette_dict.values():\n",
    "        # select only given number of colors\n",
    "        back_cols = back_cols[:output_x_num]\n",
    "        fore_cols = fore_cols[:output_y_num]\n",
    "        \n",
    "        # generate a new image - Image.new(mode,size,color)\n",
    "        im_size = (1000, 1)\n",
    "        tmp_im = Image.new('RGB', im_size, (0,0,0))\n",
    "        \n",
    "        # colorize the image - using background & forground colors\n",
    "        draw = ImageDraw.Draw(tmp_im)\n",
    "        batch = int( im_size[0] / (output_x_num + output_y_num) )\n",
    "        for idx, color in enumerate(back_cols + fore_cols):\n",
    "            color = tuple(color)  # due to PIL datatype requirement\n",
    "            draw.rectangle( xy=(idx*batch, 0, (idx+1)*batch, im_size[1]), fill=color )\n",
    "        \n",
    "        # append the colorized image\n",
    "        dataAB.append(tmp_im)\n",
    "        \n",
    "    # random.shuffle(dataAB)  # shuffle dataset before train-validation-split\n",
    "    trainAB = dataAB[ :int(len(dataAB) * 0.85) ]\n",
    "    valAB = dataAB[ int(len(dataAB) * 0.85): ]\n",
    "    return trainAB, valAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hide_y_cols(img_obj, x_num=3, y_num=2):\n",
    "    img_fake = img_obj.copy()\n",
    "    \n",
    "    draw_fake = ImageDraw.Draw(img_fake)\n",
    "    batch = int(img_fake.size[0] / (x_num+y_num))\n",
    "    for index in range(x_num, x_num+y_num):\n",
    "        draw_fake.rectangle( xy=(index*batch, 0, (index+1)*batch, img_fake.size[1]), fill=(0,0,0) )\n",
    "    \n",
    "    img_fake_arr = np.array(img_fake.resize((256,1))) / 255 * 2 - 1\n",
    "    img_real_arr = np.array(img_obj.resize((256,1))) / 255 * 2 - 1\n",
    "    return img_fake_arr, img_real_arr\n",
    "\n",
    "\n",
    "def minibatch(dataAB, batchsize, input_x_num=3, input_y_num=2):\n",
    "    length = len(dataAB) * 10\n",
    "    epoch = 0\n",
    "    \n",
    "    tmp_start = 0\n",
    "    tmp_size = None\n",
    "    while True:\n",
    "        size = tmp_size if tmp_size else batchsize\n",
    "        \n",
    "        # increase epoch\n",
    "        if (tmp_start + size) > length:\n",
    "            # random.shuffle(dataAB)\n",
    "            tmp_start = 0\n",
    "            epoch += 1\n",
    "        \n",
    "        dataA = []\n",
    "        dataB = []\n",
    "        for idx in range(tmp_start, tmp_start + size):\n",
    "            idx = int(idx % len(dataAB))\n",
    "            img_fake_arr, img_real_arr = hide_y_cols( dataAB[idx], input_x_num, input_y_num )\n",
    "            dataA.append(img_fake_arr)\n",
    "            dataB.append(img_real_arr)\n",
    "        \n",
    "        dataA = np.float32(dataA)\n",
    "        dataB = np.float32(dataB)\n",
    "        tmp_start += size\n",
    "        \n",
    "        tmp_size = yield epoch, dataA, dataB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showX(X, epoch=0, rows=3, savefig=True):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    X = np.concatenate(X,1)\n",
    "    for i in range(rows):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.imshow(np.array(Image.fromarray(((X[i]+1)/2*255).clip(0,255).astype('uint8')).resize((512,100))))\n",
    "    \n",
    "    if savefig:\n",
    "        if not os.path.exists('results/'):\n",
    "            os.makedirs('results/')\n",
    "        plt.savefig(f'results/epoch_{epoch}.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> finished loading 32 palettes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainAB, valAB = json_to_dataset(account_name)\n",
    "\n",
    "train_batch = minibatch(trainAB, batchsize=6)\n",
    "_, trainA, trainB = next(train_batch)\n",
    "\n",
    "showX([trainA, trainB], savefig=False)\n",
    "\n",
    "del train_batch, trainA, trainB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models built \n",
    "Pix2pix's network structure is used for training. Some differences are in the input size. I use 256 1 instead of 256 256 in the image . The input becomes smaller, so I increased the number of channels. By default, the first layer uses 128. Channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal\n",
    "import keras.backend as K\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import SVG\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic discriminator\n",
    "conv_init = RandomNormal(0, 0.02)\n",
    "gamma_init = RandomNormal(1., 0.02) # for batch normalization\n",
    "\n",
    "\n",
    "def conv2d(f, *a, **k):\n",
    "    return Conv2D(f, kernel_initializer = conv_init, *a, **k)\n",
    "def batchnorm():\n",
    "    return BatchNormalization(momentum=0.9, axis=-1, epsilon=1.01e-5,\n",
    "                                   gamma_initializer = gamma_init)\n",
    "def BASIC_D(nc_in=3, nc_out=3, ndf=128, max_layers=3):\n",
    "    \"\"\"DCGAN_D(nc, ndf, max_layers=3)\n",
    "       nc: channels\n",
    "       ndf: filters of the first layer\n",
    "       max_layers: max hidden layers\n",
    "    \"\"\"    \n",
    "\n",
    "    input_a, input_b = Input(shape=(None, None, nc_in)), Input(shape=(None, None, nc_out))\n",
    "    _ = Concatenate(axis=-1)([input_a, input_b])\n",
    "    _ = conv2d(ndf, kernel_size=(1,4), strides=(1,2), padding=\"same\", name = 'First') (_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    for layer in range(1, max_layers):        \n",
    "        out_feat = ndf * min(2**layer, 8)\n",
    "        _ = conv2d(out_feat, kernel_size=(1,4), strides=(1,2), padding=\"same\", \n",
    "                   use_bias=False, name = f'pyramid.{layer}'\n",
    "                        ) (_)\n",
    "        _ = batchnorm()(_, training=1)        \n",
    "        _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    out_feat = ndf*min(2**max_layers, 8)\n",
    "    _ = ZeroPadding2D((0,1))(_)\n",
    "    _ = conv2d(out_feat, kernel_size=(1,4),  use_bias=False, name = 'pyramid_last') (_)\n",
    "    _ = batchnorm()(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    # final layer\n",
    "    _ = ZeroPadding2D((0,1))(_)\n",
    "    _ = conv2d(1, kernel_size=(1,4), name = 'final'.format(out_feat, 1), \n",
    "               activation = \"sigmoid\") (_)    \n",
    "    return Model(inputs=[input_a, input_b], outputs=_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNET_G(isize, nc_in=3, nc_out=3, ngf=128, fixed_input_size=True):\n",
    "    def block(x, s, nf_in, use_batchnorm=True, nf_out=None, nf_next=None, max_nf=8*ngf):\n",
    "        # print(\"block\",x,s,nf_in, use_batchnorm, nf_out, nf_next)\n",
    "        assert s>=2 and s%2==0\n",
    "        if nf_next is None:\n",
    "            nf_next = min(nf_in*2, max_nf)\n",
    "        if nf_out is None:\n",
    "            nf_out = nf_in\n",
    "        x = conv2d(nf_next, kernel_size=(1,4), strides=(1,2), use_bias=(not (use_batchnorm and s>2)),\n",
    "                   padding=\"same\", name = f'conv_{s}') (x)\n",
    "        if s>2:\n",
    "            if use_batchnorm:\n",
    "                x = batchnorm()(x, training=1)\n",
    "            x2 = LeakyReLU(alpha=0.2)(x)\n",
    "            x2 = block(x2, s//2, nf_next)\n",
    "            x = Concatenate(axis=-1)([x, x2])            \n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2DTranspose(nf_out, kernel_size=(1,4), strides=(1,2), use_bias=not use_batchnorm,\n",
    "                            kernel_initializer = conv_init,          \n",
    "                            name = f'convt.{s}')(x)        \n",
    "        x = Cropping2D(cropping=((0,0),(1,1)))(x)\n",
    "        if use_batchnorm:\n",
    "            x = batchnorm()(x, training=1)\n",
    "        if s <=8:\n",
    "            x = Dropout(0.5)(x, training=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    s = isize if fixed_input_size else None\n",
    "    \n",
    "    inputs = Input(shape=(1, s, nc_in))        \n",
    "    _ = block(inputs, s=isize, nf_in=nc_in, use_batchnorm=False, nf_out=nc_out, nf_next=ngf)\n",
    "    _ = Activation('tanh')(_)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 6 0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "First (Conv2D)                  (None, None, None, 1 3200        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           First[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pyramid.1 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 2 1024        pyramid.1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pyramid.2 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 5 2048        pyramid.2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, None, None, 5 0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pyramid_last (Conv2D)           (None, None, None, 1 2097152     zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 1 4096        pyramid_last[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, None, None, 1 0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final (Conv2D)                  (None, None, None, 1 4097        zero_padding2d_2[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 2,766,977\n",
      "Trainable params: 2,763,393\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1, 256, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_256 (Conv2D)               (None, 1, 128, 128)  1664        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1, 128, 128)  0           conv_256[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_128 (Conv2D)               (None, 1, 64, 256)   131072      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 64, 256)   1024        conv_128[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 1, 64, 256)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_64 (Conv2D)                (None, 1, 32, 512)   524288      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 32, 512)   2048        conv_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1, 32, 512)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_32 (Conv2D)                (None, 1, 16, 1024)  2097152     leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 16, 1024)  4096        conv_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 1, 16, 1024)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 1, 8, 1024)   4194304     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 1, 8, 1024)   4096        conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 1, 8, 1024)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, 1, 4, 1024)   4194304     leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1, 4, 1024)   4096        conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 1, 4, 1024)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 1, 2, 1024)   4194304     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 2, 1024)   4096        conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 1, 2, 1024)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 1, 1, 1024)   4195328     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 1, 1024)   0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "convt.2 (Conv2DTranspose)       (None, 1, 4, 1024)   4194304     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 1, 2, 1024)   0           convt.2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1, 2, 1024)   4096        cropping2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 2, 1024)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 2, 2048)   0           batch_normalization_9[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1, 2, 2048)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.4 (Conv2DTranspose)       (None, 1, 6, 1024)   8388608     activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)       (None, 1, 4, 1024)   0           convt.4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 1, 4, 1024)   4096        cropping2d_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 4, 1024)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 4, 2048)   0           batch_normalization_8[0][0]      \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1, 4, 2048)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.8 (Conv2DTranspose)       (None, 1, 10, 1024)  8388608     activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 1, 8, 1024)   0           convt.8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1, 8, 1024)   4096        cropping2d_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 8, 1024)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 8, 2048)   0           batch_normalization_7[0][0]      \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1, 8, 2048)   0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.16 (Conv2DTranspose)      (None, 1, 18, 1024)  8388608     activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 1, 16, 1024)  0           convt.16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1, 16, 1024)  4096        cropping2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1, 16, 2048)  0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1, 16, 2048)  0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.32 (Conv2DTranspose)      (None, 1, 34, 512)   4194304     activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_5 (Cropping2D)       (None, 1, 32, 512)   0           convt.32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1, 32, 512)   2048        cropping2d_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1, 32, 1024)  0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1, 32, 1024)  0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.64 (Conv2DTranspose)      (None, 1, 66, 256)   1048576     activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_6 (Cropping2D)       (None, 1, 64, 256)   0           convt.64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1, 64, 256)   1024        cropping2d_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 64, 512)   0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 64, 512)   0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.128 (Conv2DTranspose)     (None, 1, 130, 128)  262144      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_7 (Cropping2D)       (None, 1, 128, 128)  0           convt.128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 1, 128, 128)  512         cropping2d_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1, 128, 256)  0           conv_256[0][0]                   \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1, 128, 256)  0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.256 (Conv2DTranspose)     (None, 1, 258, 3)    3075        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_8 (Cropping2D)       (None, 1, 256, 3)    0           convt.256[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1, 256, 3)    0           cropping2d_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 54,440,067\n",
      "Trainable params: 54,420,355\n",
      "Non-trainable params: 19,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_filters = 128\n",
    "netD = BASIC_D(ndf=first_filters)\n",
    "\n",
    "imageSize = 256\n",
    "netG = UNET_G(isize=imageSize)\n",
    "\n",
    "netD.summary()\n",
    "netG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A = netG.input\n",
    "fake_B = netG.output\n",
    "netG_generate = K.function([real_A], [fake_B])\n",
    "\n",
    "real_B = netD.inputs[1]\n",
    "output_D_real = netD([real_A, real_B])\n",
    "output_D_fake = netD([real_A, fake_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "loss_fn = lambda output, target : -K.mean(K.log(output+1e-12)*target+K.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "loss_D_real = loss_fn(output_D_real, K.ones_like(output_D_real))\n",
    "loss_D_fake = loss_fn(output_D_fake, K.zeros_like(output_D_fake))\n",
    "loss_G_fake = loss_fn(output_D_fake, K.ones_like(output_D_fake))\n",
    "\n",
    "loss_L1 = K.mean(K.abs(fake_B-real_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\diooo\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select optimizer\n",
    "lrD = 2e-4\n",
    "loss_D = loss_D_real +loss_D_fake\n",
    "training_updates = Adam(lr=lrD, beta_1=0.5).get_updates(netD.trainable_weights,[],loss_D)\n",
    "netD_train = K.function([real_A, real_B], [loss_D/2], training_updates)\n",
    "\n",
    "lrG = 2e-4\n",
    "loss_G = loss_G_fake   + 100 * loss_L1\n",
    "training_updates = Adam(lr=lrG, beta_1=0.5).get_updates(netG.trainable_weights,[], loss_G)\n",
    "netG_train = K.function([real_A, real_B], [loss_G_fake, loss_L1], training_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "During the training process, if the training is performed in the order of a generator and a discriminator, the result will be very poor, and the generation result is very bad, so the actual training one iteration took 3 times to generate a discriminant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netG_gen(A):\n",
    "    return np.concatenate([netG_generate([A[i:i+1]])[0] for i in range(A.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "batchSize = 1\n",
    "niter = 5\n",
    "display_iters = 2000\n",
    "\n",
    "gen_iterations = 0\n",
    "errL1 = epoch = errG = 0\n",
    "errL1_sum = errG_sum = errD_sum = 0\n",
    "\n",
    "train_batch = minibatch(trainAB, batchSize)\n",
    "val_batch = minibatch(valAB, 6)\n",
    "\n",
    "current_iter = 0\n",
    "while epoch < niter: \n",
    "    epoch, trainA, trainB = next(train_batch)        \n",
    "    errD,  = netD_train([trainA, trainB])\n",
    "    errD_sum += errD\n",
    "    \n",
    "    errG, errL1 = netG_train([trainA, trainB])\n",
    "    errG_sum += errG\n",
    "    errL1_sum += errL1\n",
    "    \n",
    "    for i in range(3):        \n",
    "        _, trainA1, trainB1 = next(train_batch)\n",
    "        errG, errL1 = netG_train([trainA1, trainB1])\n",
    "        errG_sum += errG\n",
    "        errL1_sum += errL1\n",
    "    \n",
    "    gen_iterations += 1\n",
    "    if gen_iterations % display_iters == 0:\n",
    "        if gen_iterations % (5*display_iters) == 0:\n",
    "            clear_output()\n",
    "        print(f'[{epoch}/{niter}][{gen_iterations}] Loss_D: {errD_sum/display_iters} ' + \n",
    "              f'Loss_G: {errG_sum/(display_iters*4)} loss_L1: {errL1_sum/(display_iters*4)}')\n",
    "        \n",
    "        _, valA, valB = train_batch.send(6) \n",
    "        fakeB = netG_gen(valA)\n",
    "        showX([valA, valB, fakeB], gen_iterations)\n",
    "        \n",
    "        errL1_sum = errG_sum = errD_sum = 0\n",
    "        _, valA, valB = next(val_batch)\n",
    "        fakeB = netG_gen(valA)\n",
    "        showX([valA, valB, fakeB], gen_iterations)\n",
    "        \n",
    "    if epoch > current_iter:\n",
    "        current_iter += 1\n",
    "        \n",
    "        if not os.path.exists('weights/100L1_128channel_4gen/generator/'):\n",
    "            os.makedirs('weights/100L1_128channel_4gen/generator/')\n",
    "        if not os.path.exists('weights/100L1_128channel_4gen/discriminator/'):\n",
    "            os.makedirs('weights/100L1_128channel_4gen/discriminator/')\n",
    "        \n",
    "        netG.save(f'./weights/100L1_128channel_4gen/generator/epoch_{epoch}.hdf5')\n",
    "        netD.save(f'./weights/100L1_128channel_4gen/discriminator/epoch_{epoch}.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Here I trained three models based on different parameters, used here as appropriate\n",
    "# generator = load_model('weights/GAN/100L1_128channel_peisenet/generator/epoch_2.hdf5')\n",
    "# generator1 = load_model('weights/GAN/100L1_128channel_4gen/generator/epoch_10.hdf5')\n",
    "# generator2 = load_model('weights/GAN/1000L1_256channel/generator/epoch_10.hdf5')\n",
    "\n",
    "generator = load_model('weights/100L1_128channel_4gen/generator/epoch_3.hdf5')\n",
    "generator1 = load_model('weights/100L1_128channel_4gen/generator/epoch_4.hdf5')\n",
    "generator2 = load_model('weights/100L1_128channel_4gen/generator/epoch_5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in generator.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 1 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACEklEQVR4nO3csWqTYRiG4SdJMRUEdRIdFASPIYODg6O4SEDEc3AR3QVxd5TiYHFREKWggyJ4ADp1tYW6FEVsUaw0afN7AlLwRb9SuK41w/sTHkhuAul1XRcAAAD+Tn+/HwAAAOAgElMAAAAFYgoAAKBATAEAABSIKQAAgIK5vV58/HK56V/9TWfJ+GzbvtvtelndGqbres1ubk+2c2V8MZ+/rDe7uV+6lm9sbPZ/mU0m+TC+lOnXzWY3k+Rd/3sGaTqhPJ1O2h5M8mTxRdvd7nS5PDqeWcOrO7NBNiZH2h1Msr66koe3bmYwt+dH3T81f3g+p8+daXYvSQaZ5vbzN013+2DhWdPNTnaS66O5ppvdzaFszU4kaXd049NaXt+7k/6g3Wb7R49l/vyFZveSZLb6PjcWl5puduHRUvPNXhsNm2/2Z+PN/tr8lpW3r9Lrt/su1BsO0z95qtm9JPnxcTlX797/42b9MgUAAFAgpgAAAArEFAAAQIGYAgAAKBBTAAAABWIKAACgQEwBAAAUiCkAAIACMQUAAFAgpgAAAArEFAAAQIGYAgAAKBBTAAAABWIKAACgQEwBAAAUiCkAAIACMQUAAFAgpgAAAArEFAAAQIGYAgAAKBBTAAAABWIKAACgoNd13X4/AwAAwIHjlykAAIACMQUAAFAgpgAAAArEFAAAQIGYAgAAKBBTAAAABb8B03pwQ2BQDsMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 2 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACMElEQVR4nO3cPWtTcRjG4ef0tDi4CV1sHP0UjoKTBakgFJ06CILQ6uKgIFLwFUTRqtBFwcnBrWMXFUHQVRDEl0YiioIoTWprcvwCUvBB/6FwXWuG+4Q8Q34EUjVNEwAAAPydkWE/AAAAwFYkpgAAABLEFAAAQIKYAgAASBBTAAAACaObvXj88N6if/W31luNa/eWS07Gj+/f4ujM/lhfXy+2ORg08eTZ61j7uVFsc1iapqlK7rnZ/2OkqmLPxO4YG6mLbUYVcX7pUWwbK7gZEZ87H4vebETEiSMHCt9tL+Yv3Ih+v19ss9Npx8LClajrcp/n+Ph4TB08FIPBoNhmr9OJN3cWi+1FRHzYsT1OLz0uerfTU5NFb7bb68bi9dtFb3Zl5V1cvHQ26nrTr0r/VKu1K+ZOnir6Pjc+tePL/avF9iIiXqxWMfuw7M3OTO4re7Pd1bh190H0+7+Kbbbfv43L585EPVruZne2JuLY3GzRm42NbsTXV+X2IuL505cxPX/zjzfrlykAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACVXTNMN+BgAAgC3HL1MAAAAJYgoAACBBTAEAACSIKQAAgAQxBQAAkCCmAAAAEn4DPGGBQy8NzVUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 3 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACCklEQVR4nO3cMWoUYRTA8TfrEoKFYicIElLYBiy9ibXgCRT0CskV7FwiqGgVRIMggkW8gggiaGEMYQ1JdHdnPi8gAR/6QuD3a6d43zKvmP8OTNdaCwAAAP7O6LQPAAAAcBaJKQAAgAQxBQAAkCCmAAAAEsQUAABAwvikixvvP5d+6m9oEWsXTzzSvzc7jNhej+gXdSMXfdxafxbfp4dlM09La62rnGdn/4/RuXHcuHk/xsvny2ZGRHzZvBPd0nLpzNXbD0p3NiLi0WSrdm+HFteuXIjKj7kOx9M4+vA2oqv7D29/dz+2H7+K0ahuZtdajIehbF5ExNLK5dh4vVO6tw8nL0t3th9aXF+p3dnFwW7s7TyNrnB/5rM+9r4eRFd4N4dFH79+1D6P7M2P4t6L2p2dPH9TvrNrVy+V7ux8uhvf3j0p3dn+53EcfvoYlUvbd6M4Gtc+G0xn47i7ufXHH+nNFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAICErrV22mcAAAA4c7yZAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJDwG8ywekMp3IYNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 4 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACI0lEQVR4nO3cvWoUYRSA4TMbUcIGQWIhGBDURtAijRcQLMULsLJWsLBK7yWkExsLA8aAnWJcsEiihQTBwk4ECwOrG4lRV5OdfN6ACB7wWwLP005x5uew8O7ANKWUAAAA4N90xn0CAAAAB5GYAgAASBBTAAAACWIKAAAgQUwBAAAkHPrbwfsf+1U/9TcqJS5NT9UcGcPd3VjoPYtR21abOdrbi8Wbt+LbYFBt5riUUpqa8+zs/zEatbF4+158/zGsNjMiounORFT+4mj79kHVnY2IeP5kvepFtvslZs90q97are2deLy6EZ1Ovdu7s7kZG3fvRDMxUW3m1NFuzF48V21eRMTPdj/ml1eq7u3L3qvKOxtx4fThqjv76cvXWF55EZ1Ovf+df33ux4dHS9FUnHn85Im4cuNatXkREe/XenF14WHdnV1/U/139vypyao729/ajqWna1V39shoGDODd1Gaeo9z8th0nJ27XG1eRET/9WrMXZ//40V6MwUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgoSmljPscAAAADhxvpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAk/AZwTIBDJauoOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 5 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACIklEQVR4nO3cvWoVYRCA4dkQIolCFNJZK2ghWAgRm+QGks7CQrEQ1MJKsFCwES/BUkGwSaEgCHZ2FlaCogGJQcHmEFBBk+P5+7wBERxwQuB52i1m95xh4d2F7VprAQAAwL+Z2u0TAAAA2IvEFAAAQIKYAgAASBBTAAAACWIKAAAgYfpvBy88vFf6qb9Ja3F0fqFyZIyHk1h/1Ys2qbvUyWgYz+7fjZ0f38pm7pbWWlc5z87+H9OjcVx88Chmd/plMyMitrpfUbpAEbEy2KgeGWfPXynd2+FoFKunF2NS+DXX8WgQ21+/RBT+oz97vXi7thbdVN1zw/1z++LYkcNl8yIiBjMzceP5y9K9vXTtVvHOjmNl8WTpzg7729H7tB5dV/fT9re24vOTx6U7O79wKM6sLpfNi4jYfPMhrj59Ubqzl6/fKd3ZwWgc55ZPle5sf2c7Pm68L93ZuUk/jvc3oxXe26cOHIzZE0tl8yIivr9+F0s3b//xIr2ZAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJDQtdZ2+xwAAAD2HG+mAAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACBBTAEAACT8Bk++f0O7IvBDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_batch = minibatch(valAB, 1)\n",
    "for idx in range(len(valAB[:10])):  # maximum 10 loop\n",
    "    print(f'Processing palette {idx+1} ... ')\n",
    "    \n",
    "    # get fake & real array from minibatch\n",
    "    _, valA, valB = next(val_batch)\n",
    "    \n",
    "    # transform normalized array(for model input) into pixel array\n",
    "    im_fake = ((valA+1)/2*255).clip(0,255).astype('uint8')[0]\n",
    "    im = ((valB+1)/2*255).clip(0,255).astype('uint8')[0]\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    # visualize original palette & concealed palette\n",
    "    plt.subplot(1,5,1)\n",
    "    plt.imshow(np.array(Image.fromarray(im_fake).resize((512,80))))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,5,2)\n",
    "    plt.imshow(np.array(Image.fromarray(im).resize((512,80))))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # predict concealed colors\n",
    "    result = generator.predict(valA)[0]\n",
    "    result1 = generator1.predict(valA)[0]\n",
    "    result2 = generator2.predict(valA)[0]\n",
    "    \n",
    "    batch = int(256 / 5)\n",
    "    for i in range(5):\n",
    "        result[:,i*batch:(i+1)*batch,:] = np.mean(result[:,i*batch:(i+1)*batch,:],1)[0]\n",
    "        result1[:,i*batch:(i+1)*batch,:] = np.mean(result1[:,i*batch:(i+1)*batch,:],1)[0]\n",
    "        result2[:,i*batch:(i+1)*batch,:] = np.mean(result2[:,i*batch:(i+1)*batch,:],1)[0]\n",
    "    \n",
    "    # visualize predicted colors\n",
    "    plt.subplot(1,5,3)\n",
    "    res = np.array(Image.fromarray(((result+1)/2*255).clip(0,255).astype('uint8')).resize((512,80)))\n",
    "    plt.imshow(res)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,5,4)\n",
    "    res1 = np.array(Image.fromarray(((result1+1)/2*255).clip(0,255).astype('uint8')).resize((512,80)))\n",
    "    plt.imshow(res1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,5,5)\n",
    "    res2 = np.array(Image.fromarray(((result2+1)/2*255).clip(0,255).astype('uint8')).resize((512,80)))\n",
    "    plt.imshow(res2)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
